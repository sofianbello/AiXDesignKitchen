{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"VI_PifuHD.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"mount_file_id":"1Bf4ZBVEkrNztbCGxGSpXCocO6mH3X4p_","authorship_tag":"ABX9TyOPNLf3Olc1puO5MCWsvmHH"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"VPAnOW_Wm-Qp","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627724773526,"user_tz":-120,"elapsed":9425,"user":{"displayName":"S B","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgZcv9dt9hermjqRQEh3iu4kxWTxwuEX3RRHVNgPw=s64","userId":"04462587609416669725"}},"outputId":"7a1c4e38-6d56-4753-c5c1-48dcc1cab067"},"source":["#@title ## Setup Environment { display-mode: \"form\" }\n","#@markdown This line will install **pytorch3d**.\n","!pip install pytorch3d"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting pytorch3d\n","  Downloading pytorch3d-0.3.0-cp37-cp37m-manylinux1_x86_64.whl (30.0 MB)\n","\u001b[K     |████████████████████████████████| 30.0 MB 100 kB/s \n","\u001b[?25hCollecting fvcore\n","  Downloading fvcore-0.1.5.post20210730.tar.gz (49 kB)\n","\u001b[K     |████████████████████████████████| 49 kB 6.7 MB/s \n","\u001b[?25hRequirement already satisfied: torchvision>=0.4 in /usr/local/lib/python3.7/dist-packages (from pytorch3d) (0.10.0+cu102)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.4->pytorch3d) (1.19.5)\n","Requirement already satisfied: pillow>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.4->pytorch3d) (7.1.2)\n","Requirement already satisfied: torch==1.9.0 in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.4->pytorch3d) (1.9.0+cu102)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.9.0->torchvision>=0.4->pytorch3d) (3.7.4.3)\n","Collecting yacs>=0.1.6\n","  Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n","Collecting pyyaml>=5.1\n","  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n","\u001b[K     |████████████████████████████████| 636 kB 52.0 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from fvcore->pytorch3d) (4.41.1)\n","Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.7/dist-packages (from fvcore->pytorch3d) (1.1.0)\n","Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from fvcore->pytorch3d) (0.8.9)\n","Collecting iopath>=0.1.7\n","  Downloading iopath-0.1.9-py3-none-any.whl (27 kB)\n","Collecting portalocker\n","  Downloading portalocker-2.3.0-py2.py3-none-any.whl (15 kB)\n","Building wheels for collected packages: fvcore\n","  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for fvcore: filename=fvcore-0.1.5.post20210730-py3-none-any.whl size=60619 sha256=d162ca7faa0d4cee543c6c4f6b4929c5847338b2057fa152764306f1138681ef\n","  Stored in directory: /root/.cache/pip/wheels/fc/9c/12/9e9331cff3ddb5bb02a818886f0b52f5d160c54efa19d3516a\n","Successfully built fvcore\n","Installing collected packages: pyyaml, portalocker, yacs, iopath, fvcore, pytorch3d\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed fvcore-0.1.5.post20210730 iopath-0.1.9 portalocker-2.3.0 pytorch3d-0.3.0 pyyaml-5.4.1 yacs-0.1.8\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"AyTSl5POnGzJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627724777045,"user_tz":-120,"elapsed":250,"user":{"displayName":"S B","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgZcv9dt9hermjqRQEh3iu4kxWTxwuEX3RRHVNgPw=s64","userId":"04462587609416669725"}},"outputId":"3a07f149-5ea2-4976-debd-ebd4c3535339"},"source":["#@title Mount Drive { display-mode: \"form\" }\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"nvGk6gRinJNh","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627724784040,"user_tz":-120,"elapsed":4429,"user":{"displayName":"S B","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgZcv9dt9hermjqRQEh3iu4kxWTxwuEX3RRHVNgPw=s64","userId":"04462587609416669725"}},"outputId":"f446a8fe-d9f9-48d1-ee75-096afbfd4f75"},"source":["#@title  Clone repositories { display-mode: \"form\" }\n","#@markdown Paste the **path** to your working directory below:\n","\n","#@markdown (*The path **must** end with **/** for the code to work properly.*)\n","\n","\n","path = '/content/drive/MyDrive/AiXDesignKitchen/Workshop/Playground/' #@param {type:'string'}\n","%cd {path}\n","print('Navigating to Workdirectory...')\n","print('Creating \"PifuHD\"')\n","!mkdir PifuHD\n","print('Open PifuHD...')\n","%cd {path}'PifuHD/'\n","print('Cloning PifuHD...')\n","!git clone https://github.com/facebookresearch/pifuhd\n","print('Cloning lightweight-human-pose-estimation...')\n","!git clone https://github.com/Daniil-Osokin/lightweight-human-pose-estimation.pytorch.git\n","print('All done!')"],"execution_count":3,"outputs":[{"output_type":"stream","text":["/content/drive/MyDrive/AiXDesignKitchen/Workshop/Playground\n","Navigating to Workdirectory...\n","Creating \"PifuHD\"\n","Open PifuHD...\n","/content/drive/MyDrive/AiXDesignKitchen/Workshop/Playground/PifuHD\n","Cloning PifuHD...\n","Cloning into 'pifuhd'...\n","remote: Enumerating objects: 213, done.\u001b[K\n","remote: Total 213 (delta 0), reused 0 (delta 0), pack-reused 213\u001b[K\n","Receiving objects: 100% (213/213), 402.72 KiB | 1.62 MiB/s, done.\n","Resolving deltas: 100% (104/104), done.\n","Cloning lightweight-human-pose-estimation...\n","Cloning into 'lightweight-human-pose-estimation.pytorch'...\n","remote: Enumerating objects: 120, done.\u001b[K\n","remote: Counting objects: 100% (5/5), done.\u001b[K\n","remote: Compressing objects: 100% (5/5), done.\u001b[K\n","remote: Total 120 (delta 1), reused 0 (delta 0), pack-reused 115\u001b[K\n","Receiving objects: 100% (120/120), 227.79 KiB | 971.00 KiB/s, done.\n","Resolving deltas: 100% (50/50), done.\n","All done!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Rp84dPoMnZNn","colab":{"resources":{"http://localhost:8080/nbextensions/google.colab/files.js":{"data":"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","ok":true,"headers":[["content-type","application/javascript"]],"status":200,"status_text":""}},"base_uri":"https://localhost:8080/","height":106},"executionInfo":{"status":"ok","timestamp":1627724815557,"user_tz":-120,"elapsed":22944,"user":{"displayName":"S B","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgZcv9dt9hermjqRQEh3iu4kxWTxwuEX3RRHVNgPw=s64","userId":"04462587609416669725"}},"outputId":"b3fafa37-accd-4b80-9951-48064fbc9435"},"source":["#@title ## Upload Image { display-mode: \"form\" }\n","#@markdown  Run this cell to **upload your image**. Currently PNG, JPEG files are supported. \n","\n","%cd {path}'PifuHD/pifuhd/'\n","!mkdir images\n","%cd {path}'PifuHD/pifuhd/images'\n","from google.colab import files\n","filename = list(files.upload().keys())[0]"],"execution_count":4,"outputs":[{"output_type":"stream","text":["/content/drive/MyDrive/AiXDesignKitchen/Workshop/Playground/PifuHD/pifuhd\n","/content/drive/MyDrive/AiXDesignKitchen/Workshop/Playground/PifuHD/pifuhd/images\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/html":["\n","     <input type=\"file\" id=\"files-182450d7-c6e4-4a4d-ab1b-77a57aa15b7f\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-182450d7-c6e4-4a4d-ab1b-77a57aa15b7f\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script src=\"/nbextensions/google.colab/files.js\"></script> "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Saving Beyoncé_background_removed.png to Beyoncé_background_removed.png\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"qAmshI8An313","executionInfo":{"status":"ok","timestamp":1627724824914,"user_tz":-120,"elapsed":335,"user":{"displayName":"S B","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgZcv9dt9hermjqRQEh3iu4kxWTxwuEX3RRHVNgPw=s64","userId":"04462587609416669725"}}},"source":["#@title Set input and output paths { display-mode: \"form\"}\n","import os\n","\n","try:\n","  image_path = path +'PifuHD/pifuhd/images/%s' % filename\n","except:\n","  image_path = path +'PifuHD/pifuhd/sample_images/test.png' # example image\n","image_dir = os.path.dirname(image_path)\n","file_name = os.path.splitext(os.path.basename(image_path))[0]\n","\n","# output pathes\n","obj_path = path +'PifuHD/pifuhd/results/pifuhd_final/recon/result_%s_256.obj' % file_name\n","out_img_path = path +'PifuHD/pifuhd/results/pifuhd_final/recon/result_%s_256.png' % file_name\n","video_path = path +'PifuHD/pifuhd/results/pifuhd_final/recon/result_%s_256.mp4' % file_name\n","video_display_path = path +'PifuHD/pifuhd/results/pifuhd_final/result_%s_256_display.mp4' % file_name"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"xDko-tJkn7-6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627724828716,"user_tz":-120,"elapsed":2057,"user":{"displayName":"S B","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgZcv9dt9hermjqRQEh3iu4kxWTxwuEX3RRHVNgPw=s64","userId":"04462587609416669725"}},"outputId":"1e113379-d3c6-4b2c-b238-256071329193"},"source":["#@title Load pretrained pose estimation model {display-mode: \"form\"}\n","%cd {path}'PifuHD/lightweight-human-pose-estimation.pytorch/'\n","!wget https://download.01.org/opencv/openvino_training_extensions/models/human_pose_estimation/checkpoint_iter_370000.pth"],"execution_count":6,"outputs":[{"output_type":"stream","text":["/content/drive/MyDrive/AiXDesignKitchen/Workshop/Playground/PifuHD/lightweight-human-pose-estimation.pytorch\n","--2021-07-31 09:47:08--  https://download.01.org/opencv/openvino_training_extensions/models/human_pose_estimation/checkpoint_iter_370000.pth\n","Resolving download.01.org (download.01.org)... 104.65.185.105, 2600:1406:3c:397::4b21, 2600:1406:3c:38e::4b21\n","Connecting to download.01.org (download.01.org)|104.65.185.105|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 87959810 (84M) [application/octet-stream]\n","Saving to: ‘checkpoint_iter_370000.pth’\n","\n","checkpoint_iter_370 100%[===================>]  83.88M  59.1MB/s    in 1.4s    \n","\n","2021-07-31 09:47:10 (59.1 MB/s) - ‘checkpoint_iter_370000.pth’ saved [87959810/87959810]\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"PLJcWqFWoGMg","executionInfo":{"status":"ok","timestamp":1627724847756,"user_tz":-120,"elapsed":17401,"user":{"displayName":"S B","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgZcv9dt9hermjqRQEh3iu4kxWTxwuEX3RRHVNgPw=s64","userId":"04462587609416669725"}}},"source":["#@title Run PoseEstimation {display-mode: \"form\"}\n","#@markdown In this cell the image gets optimized and then the pose is estimated. Note that it only partially works if the legs are missing.\n","\n","import torch\n","import cv2\n","import numpy as np\n","from models.with_mobilenet import PoseEstimationWithMobileNet\n","from modules.keypoints import extract_keypoints, group_keypoints\n","from modules.load_state import load_state\n","from modules.pose import Pose, track_poses\n","import demo\n","\n","#-----Configurations Start ----# \n","\n","##Only Change these if you know what you are doing!\n","\n","def get_rect(net, images, height_size):\n","    net = net.eval()\n","\n","    stride = 8\n","    upsample_ratio = 4\n","    num_keypoints = Pose.num_kpts\n","    previous_poses = []\n","    delay = 33\n","    for image in images:\n","        rect_path = image.replace('.%s' % (image.split('.')[-1]), '_rect.txt')\n","        img = cv2.imread(image, cv2.IMREAD_COLOR)\n","        orig_img = img.copy()\n","        orig_img = img.copy()\n","        heatmaps, pafs, scale, pad = demo.infer_fast(net, img, height_size, stride, upsample_ratio, cpu=False)\n","\n","        total_keypoints_num = 0\n","        all_keypoints_by_type = []\n","        for kpt_idx in range(num_keypoints):  # 19th for bg\n","            total_keypoints_num += extract_keypoints(heatmaps[:, :, kpt_idx], all_keypoints_by_type, total_keypoints_num)\n","\n","        pose_entries, all_keypoints = group_keypoints(all_keypoints_by_type, pafs)\n","        for kpt_id in range(all_keypoints.shape[0]):\n","            all_keypoints[kpt_id, 0] = (all_keypoints[kpt_id, 0] * stride / upsample_ratio - pad[1]) / scale\n","            all_keypoints[kpt_id, 1] = (all_keypoints[kpt_id, 1] * stride / upsample_ratio - pad[0]) / scale\n","        current_poses = []\n","\n","        rects = []\n","        for n in range(len(pose_entries)):\n","            if len(pose_entries[n]) == 0:\n","                continue\n","            pose_keypoints = np.ones((num_keypoints, 2), dtype=np.int32) * -1\n","            valid_keypoints = []\n","            for kpt_id in range(num_keypoints):\n","                if pose_entries[n][kpt_id] != -1.0:  # keypoint was found\n","                    pose_keypoints[kpt_id, 0] = int(all_keypoints[int(pose_entries[n][kpt_id]), 0])\n","                    pose_keypoints[kpt_id, 1] = int(all_keypoints[int(pose_entries[n][kpt_id]), 1])\n","                    valid_keypoints.append([pose_keypoints[kpt_id, 0], pose_keypoints[kpt_id, 1]])\n","            valid_keypoints = np.array(valid_keypoints)\n","            \n","            if pose_entries[n][10] != -1.0 or pose_entries[n][13] != -1.0:\n","              pmin = valid_keypoints.min(0)\n","              pmax = valid_keypoints.max(0)\n","\n","              center = (0.5 * (pmax[:2] + pmin[:2])).astype(np.int)\n","              radius = int(0.65 * max(pmax[0]-pmin[0], pmax[1]-pmin[1]))\n","            elif pose_entries[n][10] == -1.0 and pose_entries[n][13] == -1.0 and pose_entries[n][8] != -1.0 and pose_entries[n][11] != -1.0:\n","              # if leg is missing, use pelvis to get cropping\n","              center = (0.5 * (pose_keypoints[8] + pose_keypoints[11])).astype(np.int)\n","              radius = int(1.45*np.sqrt(((center[None,:] - valid_keypoints)**2).sum(1)).max(0))\n","              center[1] += int(0.05*radius)\n","            else:\n","              center = np.array([img.shape[1]//2,img.shape[0]//2])\n","              radius = max(img.shape[1]//2,img.shape[0]//2)\n","\n","            x1 = center[0] - radius\n","            y1 = center[1] - radius\n","\n","            rects.append([x1, y1, 2*radius, 2*radius])\n","\n","        np.savetxt(rect_path, np.array(rects), fmt='%d')\n","\n","#-----Configurations End ----#\n","\n","\n","net = PoseEstimationWithMobileNet()\n","checkpoint = torch.load('checkpoint_iter_370000.pth', map_location='cpu')\n","load_state(net, checkpoint)\n","\n","get_rect(net.cuda(), [image_path], 512)\n"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"hu0FYVMpoNJ4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627724904823,"user_tz":-120,"elapsed":54563,"user":{"displayName":"S B","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgZcv9dt9hermjqRQEh3iu4kxWTxwuEX3RRHVNgPw=s64","userId":"04462587609416669725"}},"outputId":"4af8565c-35e4-4544-da50-c3a9ec153d80"},"source":["#@title Convert to 3D Model {display-mode: \"form\"}\n","print('Downloading pretrained model...')\n","print(' ')\n","\n","%cd {path}'PifuHD/pifuhd/'\n","!sh ./scripts/download_trained_model.sh\n","print(' ')\n","print('Download Completed.')\n","print(' ')\n","print('Converting Image to 3D-Obj...')\n","# Warning: all images with the corresponding rectangle files under -i will be processed. \n","!python -m apps.simple_test -r 256 --use_rect -i $image_dir\n","print(' ')\n","print('All done!')\n","print(' ')\n","print('You can find your results here:')\n","print(obj_path)\n","\n","# seems that 256 is the maximum resolution that can fit into Google Colab. \n","# If you want to reconstruct a higher-resolution mesh, please try with your own machine. "],"execution_count":8,"outputs":[{"output_type":"stream","text":["Downloading pretrained model...\n"," \n","/content/drive/MyDrive/AiXDesignKitchen/Workshop/Playground/PifuHD/pifuhd\n","+ mkdir -p checkpoints\n","+ cd checkpoints\n","+ wget https://dl.fbaipublicfiles.com/pifuhd/checkpoints/pifuhd.pt pifuhd.pt\n","--2021-07-31 09:47:32--  https://dl.fbaipublicfiles.com/pifuhd/checkpoints/pifuhd.pt\n","Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.22.75.142, 172.67.9.4, 104.22.74.142, ...\n","Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.22.75.142|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 1548375177 (1.4G) [application/octet-stream]\n","Saving to: ‘pifuhd.pt’\n","\n","pifuhd.pt           100%[===================>]   1.44G  58.3MB/s    in 27s     \n","\n","2021-07-31 09:47:59 (54.5 MB/s) - ‘pifuhd.pt’ saved [1548375177/1548375177]\n","\n","--2021-07-31 09:47:59--  http://pifuhd.pt/\n","Resolving pifuhd.pt (pifuhd.pt)... failed: Name or service not known.\n","wget: unable to resolve host address ‘pifuhd.pt’\n","FINISHED --2021-07-31 09:47:59--\n","Total wall clock time: 27s\n","Downloaded: 1 files, 1.4G in 27s (54.5 MB/s)\n"," \n","Download Completed.\n"," \n","Converting Image to 3D-Obj...\n","Resuming from  ./checkpoints/pifuhd.pt\n","Warning: opt is overwritten.\n","test data size:  1\n","initialize network with normal\n","initialize network with normal\n","generate mesh (test) ...\n","  0% 0/1 [00:00<?, ?it/s]./results/pifuhd_final/recon/result_Beyoncé_background_removed_256.obj\n","100% 1/1 [00:08<00:00,  8.99s/it]\n"," \n","All done!\n"," \n","You can find your results here:\n","/content/drive/MyDrive/AiXDesignKitchen/Workshop/Playground/PifuHD/pifuhd/results/pifuhd_final/recon/result_Beyoncé_background_removed_256.obj\n"],"name":"stdout"}]}]}